# FILE: src/backend/api.py - FULLY OPTIMIZED VERSION

import sys
import os
from pathlib import Path

# Ensure backend directory is in Python path FIRST
backend_dir = str(Path(__file__).parent)
if backend_dir not in sys.path:
    sys.path.insert(0, backend_dir)

from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import requests
import yaml
import json
import asyncio
import tempfile
import importlib.util
from typing import Dict, List, Any, Callable
import re
import traceback
import time
from contextlib import asynccontextmanager

# Import backend modules
from code_review import review_code, batch_fix
from debug_guide import surface_errors, debug_step
from memory import save_memory, recall_memory, manage_privacy
from intent_handler import router as intent_router
from model_manager import load_model, list_available_models, get_model_info

# --- Load Config ---
config_path = os.path.join(os.path.dirname(__file__), "config.yaml")
try:
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
except FileNotFoundError:
    print(f"‚ö†Ô∏è Config file not found at {config_path}, using defaults")
    config = {}

api_keys = config.get("api_keys", {})
fallback_order = config.get("fallback_order", ["duckduckgo", "wikipedia"])
providers = [config.get("online_search", "duckduckgo")] + fallback_order

# ============================================================================
# BULLETPROOF TOOL REGISTRY SYSTEM
# ============================================================================

class ToolRegistry:
    def __init__(self):
        self._tools: Dict[str, Callable[..., Any]] = {}
        self._initialized = False

    def register(self, name: str, func: Callable[..., Any], desc: str = "", schema: dict = None):
        """Register a tool function with the registry"""
        func.__desc__ = desc
        func.__schema__ = schema or {}
        self._tools[name] = func
        print(f"üõ†Ô∏è Registered tool: {name} (Total: {len(self._tools)})")

    def exists(self, name: str) -> bool:
        return name in self._tools

    def call(self, name: str, **kwargs) -> Any:
        if name not in self._tools:
            raise KeyError(f"Tool '{name}' not found. Available: {list(self._tools.keys())}")
        return self._tools[name](**kwargs)

    def serialize(self) -> List[dict]:
        """Serialize tools for JSON transmission"""
        try:
            return [
                {
                    "name": n,
                    "description": getattr(f, '__desc__', ''),
                    "args_schema": getattr(f, '__schema__', {})
                }
                for n, f in self._tools.items()
            ]
        except Exception as e:
            print(f"üîå Tool registry serialization error: {e}")
            return []

    def get_tool_names(self) -> List[str]:
        return list(self._tools.keys())

    def clear(self):
        """Clear all tools - should NOT be called during normal operation"""
        print(f"‚ö†Ô∏è WARNING: Clearing tool registry with {len(self._tools)} tools!")
        self._tools.clear()

    def count(self) -> int:
        return len(self._tools)

# Initialize the global tool registry ONCE
tool_registry = ToolRegistry()

def tool(name: str, desc: str = "", schema: dict = None):
    """Tool decorator that registers functions immediately"""
    def wrapper(fn):
        tool_registry.register(name, fn, desc, schema or {})
        return fn
    return wrapper

# ============================================================================
# OPTIMIZED DYNAMIC TOOL LOADING
# ============================================================================

def load_existing_tools():
    """Load tools from the tools directory - OPTIMIZED VERSION"""
    tools_dir = Path(__file__).parent / "tools"
    
    # Create tools directory if it doesn't exist
    if not tools_dir.exists():
        print("üîç Tools directory doesn't exist, creating it...")
        tools_dir.mkdir(exist_ok=True)

    print(f"üîç Loading tools from: {tools_dir}")

    # Add directories to Python path
    if str(tools_dir) not in sys.path:
        sys.path.insert(0, str(tools_dir))

    tools_loaded = 0
    for tool_file in tools_dir.glob("*.py"):
        if tool_file.name == "__init__.py":
            continue

        try:
            print(f"üì¶ Loading tool file: {tool_file}")
            tool_name = tool_file.stem

            # Read and modify the file content to inject our registry
            file_content = tool_file.read_text(encoding='utf-8')
            modified_content = file_content

            import_patterns = [
                ("from backend.api import tool", "# INJECTED: Using global registry"),
                ("from backend.api import tool, hybrid_online_search", "# INJECTED: Using global registry"),
                ("from ..api import tool", "# INJECTED: Using global registry"),
                ("from .api import tool", "# INJECTED: Using global registry"),
                ("from api import tool", "# INJECTED: Using global registry")
            ]

            for old_import, replacement in import_patterns:
                if old_import in modified_content:
                    modified_content = modified_content.replace(old_import, replacement)

            # Execute with proper globals that include our instances
            exec_globals = {
                '__name__': f'tools.{tool_name}',
                '__file__': str(tool_file),
                '__builtins__': __builtins__,
                # Inject our global instances
                'tool_registry': tool_registry,
                'tool': tool,
                'hybrid_online_search': hybrid_online_search,
                # Add common imports that tools might need
                'Path': Path,
                'os': os,
                'json': json,
                'requests': requests,
                'asyncio': asyncio,
            }

            # Create empty locals dict
            exec_locals = {}

            # Execute the modified content
            exec(modified_content, exec_globals, exec_locals)
            tools_loaded += 1
            print(f"‚úÖ Successfully loaded tool: {tool_name}")

        except Exception as e:
            print(f"‚ùå Failed to load tool {tool_file}: {e}")
            traceback.print_exc()

    print(f"üì¶ Tool loading complete: {tools_loaded} files processed")

# ============================================================================
# SEARCH PROVIDER FUNCTIONS
# ============================================================================

def search_perplexity(query):
    try:
        url = "https://api.perplexity.ai/search"
        headers = {"Authorization": f"Bearer {api_keys.get('perplexity_api_key', '')}"}
        resp = requests.post(url, json={"q": query}, headers=headers, timeout=10)
        resp.raise_for_status()
        return resp.json()["answer"]
    except Exception as e:
        return f"Perplexity search failed: {str(e)}"

def search_duckduckgo(query):
    try:
        url = f"https://api.duckduckgo.com/?q={query}&format=json"
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        answer = data.get("AbstractText", "") or data.get("Answer", "")
        return answer if answer else "No concise answer found."
    except Exception as e:
        return f"DuckDuckGo search failed: {str(e)}"

def search_wikipedia(query):
    try:
        url = f"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={query}&format=json"
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        results = resp.json().get("query", {}).get("search", [])
        return results[0]["snippet"] if results else "No Wikipedia result found."
    except Exception as e:
        return f"Wikipedia search failed: {str(e)}"

def search_wolframalpha(query):
    try:
        appid = api_keys.get("wolframalpha_appid")
        if not appid:
            return "WolframAlpha API key not configured"
        url = f"https://api.wolframalpha.com/v1/result?appid={appid}&i={query}"
        resp = requests.get(url, timeout=10)
        if resp.status_code == 501:
            return "WolframAlpha: No result for your query."
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        return f"WolframAlpha search failed: {str(e)}"

def search_open_meteo(query):
    try:
        # Simple weather query - you can enhance this with proper location parsing
        url = "https://api.open-meteo.com/v1/forecast?latitude=52.52&longitude=13.41&current_weather=true"
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        current = resp.json().get("current_weather", {})
        if current:
            temp = current.get("temperature", "unknown")
            windspeed = current.get("windspeed", "unknown")
            return f"Berlin weather: {temp}¬∞C, wind {windspeed} km/h"
        return "Weather data not available."
    except Exception as e:
        return f"Open-Meteo search failed: {str(e)}"

PROVIDER_FUNCS = {
    "perplexity": search_perplexity,
    "duckduckgo": search_duckduckgo,
    "wikipedia": search_wikipedia,
    "wolframalpha": search_wolframalpha,
    "open-meteo": search_open_meteo,
}

def hybrid_online_search(query):
    """Hybrid search that tries multiple providers"""
    last_error = ""
    for provider in providers:
        if provider in PROVIDER_FUNCS:
            try:
                result = PROVIDER_FUNCS[provider](query)
                if result and "failed:" not in result.lower() and "not available" not in result.lower():
                    return {"provider": provider, "result": result}
            except Exception as e:
                last_error = f"{provider}: {str(e)}"
                continue
    return {"error": f"No provider returned a valid result. Last error: {last_error}"}

# ============================================================================
# FIXED MODEL MANAGEMENT WITH DYNAMIC VALIDATION
# ============================================================================

# Global variables for lazy loading
_model_cache = {}
_available_models_cache = None
CURRENT_MODEL = None

def safe_list_available_models():
    """Safely list available models with caching"""
    global _available_models_cache
    
    if _available_models_cache is not None:
        return _available_models_cache
        
    try:
        models = list_available_models()
        _available_models_cache = models if models else []
        return _available_models_cache
    except Exception as e:
        print(f"‚ö†Ô∏è Model discovery failed: {e}")
        _available_models_cache = []
        return []

def is_valid_model_path(model_path):
    """FIXED: Check if a model path OR model name is valid"""
    if not model_path:
        return False
    if not isinstance(model_path, (str, os.PathLike)):
        return False
    
    # CRUCIAL FIX: Check if it's a discovered model name first
    try:
        available_models = safe_list_available_models()
        if str(model_path) in available_models:
            return True
    except Exception:
        pass
    
    # Fallback: check if it's a direct file/directory path
    try:
        return os.path.exists(str(model_path))
    except:
        return False

def lazy_load_model(model_name):
    """Lazy load model only when needed"""
    global _model_cache
    
    if model_name in _model_cache:
        return _model_cache[model_name]
    
    try:
        print(f"ü§ñ Lazy loading model: {model_name}")
        tokenizer, model = load_model(model_name)
        _model_cache[model_name] = (tokenizer, model)
        return tokenizer, model
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to lazy load model {model_name}: {e}")
        return None, None

def validate_current_model():
    """Validate and set current model"""
    global CURRENT_MODEL
    if not is_valid_model_path(CURRENT_MODEL):
        print(f"‚ö†Ô∏è Invalid CURRENT_MODEL: {CURRENT_MODEL}, resetting to None")
        CURRENT_MODEL = None

# FIXED: Initialize model system with proper auto-selection
try:
    available_models = safe_list_available_models()
    CURRENT_MODEL = config.get("model")
    
    # Auto-select first available model if config is null or invalid
    if not CURRENT_MODEL and available_models:
        CURRENT_MODEL = available_models[0]
        print(f"ü§ñ Auto-selected model: {CURRENT_MODEL}")
    
    # Validate the selection (with fixed validation function)
    validate_current_model()
    
    # Final fallback: if still invalid, try first available again
    if not CURRENT_MODEL and available_models:
        CURRENT_MODEL = available_models[0]
        print(f"ü§ñ Fallback to first available model: {CURRENT_MODEL}")
        
    print(f"ü§ñ Model initialization: Found {len(available_models)} models, current: {CURRENT_MODEL}")
        
except Exception as e:
    print(f"‚ö†Ô∏è Model initialization failed: {e}")
    available_models = []
    CURRENT_MODEL = None

# ============================================================================
# CONVERSATION MODE DETECTION
# ============================================================================

def should_use_tool_mode(message: str) -> bool:
    """Determine if we should use tool mode or conversation mode"""
    message_lower = message.lower()
    
    # Discussion patterns suggest conversation mode
    discussion_patterns = [
        "i think", "i believe", "i'm worried", "i'm concerned",
        "this might", "this could", "this may", "what if",
        "do you think", "opinion", "thoughts on", "feelings about",
        "problem with", "issue with", "concerns about", "discuss"
    ]
    
    # Command patterns suggest tool mode
    command_patterns = [
        "read the", "search for", "find the", "analyze the", "check the",
        "show me", "help me", "can you", "please", "go ahead and"
    ]
    
    if any(pattern in message_lower for pattern in discussion_patterns):
        return False
    if any(pattern in message_lower for pattern in command_patterns):
        return True
    
    # Default to tool mode if we have a valid model, conversation mode otherwise
    return is_valid_model_path(CURRENT_MODEL)

def build_react_prompt_with_tools(message: str, context: dict, tools: List[dict]) -> str:
    """Build a ReAct-style prompt with available tools"""
    workspace_folders = context.get('workspace_folders', [])
    active_file = context.get('active_file', 'None')
    
    return f"""You are AIDE, an autonomous coding assistant.

IMPORTANT: Only take Action if the user is giving you a direct command or request.
If they're discussing, asking opinions, or expressing concerns, respond conversationally without invoking tools.

Workspace Folders: {workspace_folders}
Active File: {active_file}

Tools Available: {json.dumps(tools, indent=2)}

User Query: {message}

If this is a COMMAND or REQUEST:
Thought: [analyze what action is needed]
Action: invoke.toolName{{"arg": "value"}}

If this is DISCUSSION or ANALYSIS:
Thought: [provide thoughtful response without tools]
Response: [conversational reply]

Current query analysis: {message}"""

# ============================================================================
# AGENTIC INTENT PROCESSOR
# ============================================================================

class AgenticIntentProcessor:
    def __init__(self):
        self.intent_patterns = {
            "code_review": ["review", "check", "analyze", "look at", "examine"],
            "fix_bugs": ["fix", "debug", "error", "bug", "issue", "problem"],
            "explain": ["explain", "what does", "how does", "describe", "tell me about"],
            "generate": ["create", "generate", "write", "make", "build"],
            "test": ["test", "testing", "unit test", "spec"],
            "refactor": ["refactor", "improve", "optimize", "clean up"],
            "document": ["document", "docs", "documentation", "comment"],
            "search": ["find", "search", "look for", "locate"]
        }

    def process_intent(self, message: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Process user intent and return appropriate response"""
        message_lower = message.lower()
        detected_intents = []
        
        for intent, keywords in self.intent_patterns.items():
            if any(keyword in message_lower for keyword in keywords):
                detected_intents.append(intent)
        
        if not detected_intents:
            detected_intents = ["general_help"]
        
        response_parts = []
        suggested_actions = []
        
        for intent in detected_intents:
            intent_response = self._handle_intent(intent, message, context)
            response_parts.append(intent_response["response"])
            suggested_actions.extend(intent_response["actions"])
        
        return {
            "response": "\n\n".join(response_parts),
            "actions": suggested_actions,
            "detected_intents": detected_intents
        }

    def _handle_intent(self, intent: str, message: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Handle specific intent"""
        current_file = context.get("currentFile", {})
        workspace = context.get("workspace", {})
        
        responses = {
            "code_review": {
                "response": f"I'll help you review the code. {self._get_file_context_message(current_file)}",
                "actions": [
                    {"type": "code_review", "label": "üîç Start Code Review"},
                    {"type": "batch_fix", "label": "üõ†Ô∏è Auto-fix Issues"}
                ]
            },
            "fix_bugs": {
                "response": f"I'll help you identify and fix bugs. {self._get_file_context_message(current_file)}",
                "actions": [
                    {"type": "debug_guide", "label": "üêõ Debug Guide"},
                    {"type": "batch_fix", "label": "üîß Batch Fix"}
                ]
            },
            "search": {
                "response": "I can search through your codebase, documentation, or the web for information.",
                "actions": [
                    {"type": "online_search", "label": "üåê Web Search"}
                ]
            },
            "general_help": {
                "response": f"I'm AIDE, your intelligent coding assistant! I can help with code review, debugging, testing, documentation, and more. {self._get_workspace_context_message(workspace, current_file)}",
                "actions": [
                    {"type": "code_review", "label": "üîç Review Current File"},
                    {"type": "debug_guide", "label": "üêõ Debug Help"},
                    {"type": "batch_fix", "label": "üõ†Ô∏è Fix Issues"}
                ]
            }
        }
        
        return responses.get(intent, responses["general_help"])

    def _get_file_context_message(self, current_file: Dict[str, Any]) -> str:
        """Get context message for current file"""
        if current_file and current_file.get("filename"):
            filename = current_file["filename"].split("/")[-1]
            language = current_file.get("language", "unknown")
            if current_file.get("selection"):
                return f"I can see you have selected code in {filename} ({language}). Let me analyze that selection."
            else:
                return f"I can see you're working on {filename} ({language}). Let me analyze the entire file."
        return "Please open a file in the editor so I can provide more specific assistance."

    def _get_workspace_context_message(self, workspace: Dict[str, Any], current_file: Dict[str, Any]) -> str:
        """Get context message for workspace"""
        messages = []
        if workspace and workspace.get("name"):
            messages.append(f"I can see you're working in the '{workspace['name']}' workspace.")
        if current_file and current_file.get("filename"):
            filename = current_file["filename"].split("/")[-1]
            messages.append(f"Currently viewing: {filename}")
        return " ".join(messages) if messages else "Open a workspace and file to get started!"

agentic_processor = AgenticIntentProcessor()

# ============================================================================
# OPTIMIZED LLM CONVERSATION CODE WITH LAZY LOADING
# ============================================================================

async def api_chat_internal(message: str, context: dict) -> dict:
    """Internal chat API for conversation mode with lazy loading"""
    system_prompt = """I'm AIDE, your intelligent coding assistant. I can help with code review, debugging, testing, documentation, and more. I have access to tools and can provide both conversational responses and technical assistance."""

    if CURRENT_MODEL and is_valid_model_path(CURRENT_MODEL):
        try:
            # Lazy load the model
            tokenizer, model = lazy_load_model(CURRENT_MODEL)
            if tokenizer and model:
                input_prompt = f"{system_prompt}\n\nUser: {message}\nAIDE:"
                input_data = tokenizer(input_prompt, return_tensors="pt", truncation=True, max_length=2048)
                
                if hasattr(model, 'device'):
                    input_data = {k: v.to(model.device) for k, v in input_data.items()}
                
                output_tokens = model.generate(
                    **input_data,
                    max_new_tokens=512,
                    do_sample=True,
                    temperature=0.8,
                    top_p=0.95,
                    pad_token_id=tokenizer.eos_token_id
                )
                
                response_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
                if "AIDE:" in response_text:
                    response_text = response_text.split("AIDE:")[-1].strip()
                
                return {"response": response_text, "type": "conversation"}
            
        except Exception as e:
            print(f"‚ö†Ô∏è Conversational model failed: {e}")
    
    # Enhanced fallback with context awareness
    return {
        "response": f"I understand you're asking about: {message}. I can help with code analysis, debugging, file operations, and web searches. What would you like me to do?",
        "type": "conversation"
    }

async def generate_with_tool_calling(model, tokenizer, message, context):
    """Generate response with tool calling capability"""
    try:
        # Get available tools safely
        try:
            available_tools = tool_registry.serialize()
        except Exception as e:
            print(f"‚ö†Ô∏è Tool serialization in generate_with_tool_calling failed: {e}")
            available_tools = []
        
        search_tools = list(PROVIDER_FUNCS.keys())
        current_file = context.get("currentFile", {})
        workspace = context.get("workspace", {})
        
        system_prompt = (
            "You are AIDE, an advanced coding assistant. "
            f"Available tools: {json.dumps(available_tools, indent=2)}\n"
            f"Search providers: {search_tools}\n"
            f"Workspace: {workspace.get('name', 'None')}\n"
            f"File: {current_file.get('filename', 'None')}\n"
            "Use TOOL[tool_name] to invoke tools.\n\n"
            f"User: {message}\nAIDE:"
        )
        
        input_data = tokenizer(system_prompt, return_tensors="pt", truncation=True, max_length=2048)
        if hasattr(model, 'device'):
            input_data = {k: v.to(model.device) for k, v in input_data.items()}
        
        output_tokens = model.generate(
            **input_data,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.8,
            top_p=0.95,
            pad_token_id=tokenizer.eos_token_id
        )
        
        response_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
        if "AIDE:" in response_text:
            response_text = response_text.split("AIDE:")[-1].strip()
        
        # Look for tool invocations
        tool_pattern = re.compile(r"TOOL\[(\w+)\]", re.I)
        tools_found = tool_pattern.findall(response_text)
        used_tools = []
        actions = []
        
        for tool in set(tools_found):
            if tool.lower() in PROVIDER_FUNCS:
                try:
                    result = PROVIDER_FUNCS[tool.lower()](message)
                    used_tools.append(tool)
                    actions.append({"type": "search_tool", "tool": tool, "result": result})
                    response_text += f"\n\n**{tool} Result:**\n{result}"
                except Exception as e:
                    response_text += f"\n\n*{tool} error: {str(e)}*"
            elif tool_registry.exists(tool):
                try:
                    result = tool_registry.call(tool)
                    used_tools.append(tool)
                    actions.append({"type": "custom_tool", "tool": tool, "result": result})
                    response_text += f"\n\n**{tool} Result:**\n{json.dumps(result, indent=2)}"
                except Exception as e:
                    response_text += f"\n\n*{tool} error: {str(e)}*"
        
        return response_text, used_tools, actions
        
    except Exception as e:
        error_msg = f"Model generation failed: {str(e)}"
        print(f"‚ö†Ô∏è {error_msg}")
        return error_msg, [], []

# ============================================================================
# FULLY OPTIMIZED LIFESPAN - THE KEY FIX
# ============================================================================

# Global variables that are referenced later
active_connections = set()
shutdown_event = asyncio.Event()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup - FULLY OPTIMIZED AND NON-BLOCKING
    print("üöÄ AIDE Backend starting with FULLY OPTIMIZED bulletproof tool loading...")
    
    # NON-BLOCKING: Replace time.sleep with await asyncio.sleep
    await asyncio.sleep(0.05)  # Minimal delay for system stabilization
    
    # Load tools from directory
    print(f"üìä Pre-loading tool count: {len(tool_registry.get_tool_names())}")
    print(f"üìã Pre-loading tools: {tool_registry.get_tool_names()}")
    
    load_existing_tools()
    
    # NON-BLOCKING: Give tools time to register
    await asyncio.sleep(0.05)  # Minimal delay
    
    # Final verification with detailed debugging
    final_tool_count = len(tool_registry.get_tool_names())
    final_tool_names = tool_registry.get_tool_names()
    
    print(f"üõ†Ô∏è FINAL TOOL COUNT: {final_tool_count} tools registered")
    print(f"üìã FINAL TOOL NAMES: {final_tool_names}")
    
    # Debug the registry state
    print(f"üîç Registry internal state: {len(tool_registry._tools)} tools in _tools dict")
    print(f"üîç Registry keys: {list(tool_registry._tools.keys())}")
    print(f"üîå WebSocket enabled with generous timeout handling")
    print(f"ü§ñ Model system: {'‚úÖ Ready with ' + str(CURRENT_MODEL) if is_valid_model_path(CURRENT_MODEL) else '‚ö†Ô∏è No valid model - fallback mode'}")
    
    yield  # This is where the app runs
    
    # Shutdown (optional cleanup)
    print("üõë AIDE Backend shutting down gracefully...")

# --- FastAPI app with fully optimized lifespan ---
app = FastAPI(
    title="AIDE Backend - FULLY OPTIMIZED STARTUP VERSION",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(intent_router, prefix="/api/v1")

# ============================================================================
# WEBSOCKET ENDPOINT - FULLY OPTIMIZED
# ============================================================================

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    try:
        print("üîå WebSocket connection attempt starting...")
        await websocket.accept(extra_headers=[
            ("Access-Control-Allow-Origin", "*"),
            ("Access-Control-Allow-Credentials", "true")
        ])
        print("üîå WebSocket accepted successfully!")
        
        # Add to active connections
        active_connections.add(websocket)
        
        # Minimal startup delay - OPTIMIZED
        await asyncio.sleep(0.05)  # Reduced to minimal
        
        # Get tools and models safely
        try:
            tools = tool_registry.serialize()
            print(f"üîå Tool registry: {len(tools)} tools found")
            print(f"üîå Tool names: {tool_registry.get_tool_names()}")
        except Exception as e:
            print(f"üîå Tool registry serialization failed: {e}")
            traceback.print_exc()
            tools = []
        
        try:
            models = safe_list_available_models()
            print(f"üîå Models: {len(models)} found")
        except Exception as e:
            print(f"üîå Model listing error: {e}")
            traceback.print_exc()
            models = []
        
        try:
            current = CURRENT_MODEL if is_valid_model_path(CURRENT_MODEL) else None
            print(f"üîå Current model: {current}")
        except Exception as e:
            print(f"üîå Current model error: {e}")
            traceback.print_exc()
            current = None
        
        # Send initial registry message
        try:
            initial_message = {
                "type": "registry",
                "tools": tools,
                "workspace_context": {
                    "available_models": models,
                    "current_model": current,
                    "total_tools": len(tools),
                    "model_status": "ready" if current else "no_models_available"
                }
            }
            await websocket.send_json(initial_message)
            print("üîå Initial message sent successfully! üéâ")
        except Exception as e:
            print(f"üîå Failed to send initial message: {e}")
            traceback.print_exc()
            # Send a minimal fallback message
            await websocket.send_json({
                "type": "registry",
                "tools": [],
                "message": "Connected with limited functionality"
            })
        
        # Message processing loop
        while True:
            try:
                data = await asyncio.wait_for(websocket.receive_json(), timeout=30.0)
                msg_type = data.get("type")
                
                if msg_type == "query":
                    message = data.get("message", "")
                    context = data.get("context", {})
                    print(f"ü§ñ Processing query: {message[:50]}...")
                    
                    if should_use_tool_mode(message):
                        print("üõ†Ô∏è Using tool mode")
                        
                        # Build enhanced prompt with tools
                        try:
                            enhanced_prompt = build_react_prompt_with_tools(message, context, tool_registry.serialize())
                        except Exception as e:
                            print(f"üîå Tool serialization for prompt failed: {e}")
                            enhanced_prompt = f"You are AIDE. User: {message}"
                        
                        if is_valid_model_path(CURRENT_MODEL):
                            try:
                                # LAZY LOAD model
                                tokenizer, model = lazy_load_model(CURRENT_MODEL)
                                if tokenizer and model:
                                    response, used_tools, actions = await generate_with_tool_calling(model, tokenizer, enhanced_prompt, context)
                                    mode = "tool"
                                else:
                                    response = "Model failed to load. Using enhanced fallback mode."
                                    used_tools = []
                                    actions = []
                                    mode = "tool_fallback"
                                    
                            except Exception as model_err:
                                print(f"‚ö†Ô∏è Model failed: {model_err}")
                                response = f"AI model encountered an issue: {str(model_err)}. Using enhanced fallback mode."
                                used_tools = []
                                actions = []
                                mode = "tool_fallback"
                        else:
                            response = "No AI model available. Using enhanced fallback mode - I can still help with file operations and web searches!"
                            used_tools = []
                            actions = []
                            mode = "tool_fallback"
                        
                        response_data = {
                            "response": response,
                            "tools_invoked": used_tools,
                            "actions": actions
                        }
                    
                    else:
                        print("üí¨ Using chat mode")
                        response_data = await api_chat_internal(message, context)
                        mode = "chat"
                    
                    await websocket.send_json({
                        "type": "response",
                        "data": response_data,
                        "mode": mode
                    })
                
                elif msg_type == "invoke":
                    tool_name = data.get("tool")
                    args = data.get("args", {})
                    print(f"üîß Invoking tool: {tool_name}")
                    
                    if tool_registry.exists(tool_name):
                        try:
                            result = tool_registry.call(tool_name, **args)
                            await websocket.send_json({
                                "type": "tool_response",
                                "tool": tool_name,
                                "result": result
                            })
                        except Exception as e:
                            await websocket.send_json({
                                "type": "error",
                                "message": f"Tool {tool_name} failed: {str(e)}"
                            })
                    else:
                        await websocket.send_json({
                            "type": "error",
                            "message": f"Tool {tool_name} not found. Available: {tool_registry.get_tool_names()}"
                        })
                
                elif msg_type == "propose_new_tool":
                    try:
                        name = data.get("name")
                        code = data.get("code")
                        print(f"üèóÔ∏è Creating tool: {name}")
                        
                        # Create tool file
                        tools_dir = Path(__file__).parent / "tools"
                        tools_dir.mkdir(exist_ok=True)
                        tool_file = tools_dir / f"{name}.py"
                        tool_file.write_text(code, encoding="utf-8")
                        
                        await asyncio.sleep(0.05)
                        
                        # Execute the new tool file
                        spec = importlib.util.spec_from_file_location(name, str(tool_file))
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                        
                        # Send updated registry
                        try:
                            serialized_tools = tool_registry.serialize()
                        except Exception as e:
                            print(f"üîå Tool serialization for new tool response failed: {e}")
                            serialized_tools = []
                        
                        await websocket.send_json({
                            "type": "registry",
                            "tools": serialized_tools,
                            "message": f"Tool '{name}' created successfully",
                            "workspace_context": {
                                "available_models": safe_list_available_models(),
                                "current_model": CURRENT_MODEL if is_valid_model_path(CURRENT_MODEL) else None,
                                "total_tools": len(tool_registry.get_tool_names())
                            }
                        })
                        
                    except Exception as e:
                        print(f"‚ùå Tool creation failed: {e}")
                        traceback.print_exc()
                        await websocket.send_json({
                            "type": "error",
                            "message": f"Failed to create tool: {str(e)}"
                        })
                
            except asyncio.TimeoutError:
                print("‚è∞ WebSocket message timeout - client may be slow")
                continue
            except json.JSONDecodeError as e:
                await websocket.send_json({
                    "type": "error",
                    "message": f"Invalid JSON: {str(e)}"
                })
            except Exception as e:
                print(f"‚ùå Message processing error: {e}")
                traceback.print_exc()
                try:
                    await websocket.send_json({
                        "type": "error",
                        "message": f"Processing failed: {str(e)}"
                    })
                except:
                    print("üîå Failed to send error - connection may be dead")
    
    except WebSocketDisconnect:
        print("üîå WebSocket disconnected normally")
    except Exception as e:
        print(f"üîå CRITICAL WebSocket error: {e}")
        traceback.print_exc()
    finally:
        # Remove from active connections
        active_connections.discard(websocket)

# ============================================================================
# REST API ENDPOINTS - ENHANCED WITH BETTER MODEL HANDLING
# ============================================================================

@app.get("/health")
async def health():
    return {
        "status": "ok",
        "message": "AIDE backend running - FULLY OPTIMIZED STARTUP",
        "websocket_enabled": True,
        "tools_registered": len(tool_registry.get_tool_names()),
        "current_model": CURRENT_MODEL,
        "model_valid": is_valid_model_path(CURRENT_MODEL),
        "startup_optimized": True,
        "model_count": len(safe_list_available_models())
    }

@app.get("/health/websocket")
async def websocket_health():
    """Health check specifically for WebSocket functionality"""
    try:
        tools = tool_registry.serialize()
        models = safe_list_available_models()
        current_model = CURRENT_MODEL if is_valid_model_path(CURRENT_MODEL) else None
        
        return {
            "status": "ok",
            "websocket_ready": True,
            "tools_count": len(tools),
            "models_count": len(models),
            "current_model": current_model,
            "message": "WebSocket endpoint is ready - FULLY OPTIMIZED"
        }
    except Exception as e:
        return {
            "status": "error",
            "websocket_ready": False,
            "error": str(e),
            "message": "WebSocket endpoint has issues"
        }

@app.get("/models")
async def api_list_models():
    models = safe_list_available_models()
    return {
        "models": models,
        "current": CURRENT_MODEL,
        "total_available": len(models),
        "discovery_method": "filesystem_scan",
        "current_valid": is_valid_model_path(CURRENT_MODEL),
        "lazy_loading": True
    }

@app.post("/models/use")
async def api_choose_model(request: Request):
    global CURRENT_MODEL
    data = await request.json()
    model_name = data.get("name")
    
    if not model_name:
        return {"error": "No model name provided"}
    
    available_models = safe_list_available_models()
    if model_name not in available_models:
        return {
            "error": f"Model '{model_name}' not found",
            "available": available_models,
            "suggestion": "Check models/ directory"
        }
    
    try:
        CURRENT_MODEL = model_name
        validate_current_model()
        
        if is_valid_model_path(CURRENT_MODEL):
            # Clear cache to force reload
            global _model_cache
            if CURRENT_MODEL in _model_cache:
                del _model_cache[CURRENT_MODEL]
            
            # Test lazy load the model
            tokenizer, model = lazy_load_model(CURRENT_MODEL)
            if tokenizer and model:
                print(f"‚úÖ Model loaded: {model_name}")
                return {
                    "status": "success",
                    "active": CURRENT_MODEL,
                    "message": f"Successfully switched to {model_name}",
                    "lazy_loaded": True
                }
            else:
                return {
                    "error": f"Failed to lazy load model: {model_name}",
                    "current": CURRENT_MODEL
                }
        else:
            return {
                "error": f"Model path invalid: {model_name}",
                "current": CURRENT_MODEL
            }
    except Exception as e:
        return {
            "error": f"Failed to load model {model_name}: {str(e)}",
            "current": CURRENT_MODEL
        }

@app.get("/models/info/{model_name}")
async def api_model_info(model_name: str):
    try:
        info = get_model_info(model_name)
        return info if info else {"error": f"Model '{model_name}' not found"}
    except Exception as e:
        return {"error": f"Failed to get model info: {str(e)}"}

@app.get("/models/current")
async def api_current_model():
    if not is_valid_model_path(CURRENT_MODEL):
        return {
            "error": "No valid model active",
            "available_models": safe_list_available_models(),
            "suggestion": "Use POST /models/use to activate a model"
        }
    
    return {
        "current_model": CURRENT_MODEL,
        "status": "active",
        "valid": True,
        "lazy_loaded": CURRENT_MODEL in _model_cache
    }

@app.post("/chat")
async def api_chat(request: Request):
    data = await request.json()
    message = data.get("message", "")
    context = data.get("context", {})
    
    if not message:
        return {"error": "No message provided"}
    
    try:
        if is_valid_model_path(CURRENT_MODEL):
            try:
                print(f"ü§ñ Using model: {CURRENT_MODEL}")
                # Lazy load the model
                tokenizer, model = lazy_load_model(CURRENT_MODEL)
                if tokenizer and model:
                    response, used_tools, actions = await generate_with_tool_calling(model, tokenizer, message, context)
                    return {
                        "response": response,
                        "model_used": CURRENT_MODEL,
                        "actions": actions,
                        "tools_invoked": used_tools,
                        "conversation_type": "llm_first",
                        "lazy_loaded": True
                    }
                else:
                    raise Exception("Model lazy loading failed")
                    
            except Exception as model_err:
                print(f"‚ö†Ô∏è Model failed: {str(model_err)}")
                result = agentic_processor.process_intent(message, context)
                result["fallback_reason"] = f"Model error: {str(model_err)}"
                result["conversation_type"] = "enhanced_fallback"
                return result
        else:
            print("üìù No valid model, using enhanced processor")
            result = agentic_processor.process_intent(message, context)
            result["fallback_reason"] = "No valid model loaded"
            result["conversation_type"] = "enhanced_fallback"
            
            # Add web search for relevant queries
            search_keywords = ["search", "find", "look up", "what is", "who is", "when did", "how to"]
            if any(keyword in message.lower() for keyword in search_keywords):
                search_result = hybrid_online_search(message)
                if "result" in search_result:
                    result["response"] += f"\n\nüåê **Web Search:**\n{search_result['result']}"
            
            return result
            
    except Exception as e:
        return {
            "response": f"I encountered an error: {str(e)}",
            "actions": [],
            "conversation_type": "error"
        }

@app.post("/agentic-intent")
async def api_agentic_intent(request: Request):
    return await api_chat(request)

@app.post("/online-search")
async def api_online_search(request: Request):
    payload = await request.json()
    query = payload.get("query", "")
    result = hybrid_online_search(query)
    return result

@app.post("/review-code")
async def api_review_code(request: Request):
    data = await request.json()
    return review_code(data)

@app.post("/batch-fix")
async def api_batch_fix(request: Request):
    data = await request.json()
    return batch_fix(data)

@app.post("/debug-guide")
async def api_debug_guide(request: Request):
    data = await request.json()
    return surface_errors(data)

@app.post("/debug-step")
async def api_debug_step(request: Request):
    data = await request.json()
    return debug_step(data)

@app.post("/memory")
async def api_save_memory(request: Request):
    data = await request.json()
    return save_memory(data)

@app.get("/memory/recall")
async def api_recall_memory():
    return recall_memory()

@app.post("/memory/privacy")
async def api_manage_privacy(request: Request):
    data = await request.json()
    return manage_privacy(data)

@app.post("/ingest")
async def api_ingest_document(request: Request):
    data = await request.json()
    file_path = data.get("file_path")
    file_name = data.get("file_name")
    
    if not file_path:
        return {"error": "No file path provided"}
    
    try:
        return {
            "status": "success",
            "message": f"Successfully ingested {file_name}",
            "file_path": file_path,
            "file_name": file_name
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Failed to ingest {file_name}: {str(e)}"
        }

# [Include all your existing speech endpoints - they're working fine]

@app.post("/force-shutdown")
async def force_shutdown():
    """Force shutdown endpoint that kills the process"""
    import os
    import signal
    
    print("üõë FORCE SHUTDOWN REQUESTED - TERMINATING NOW")
    
    # Close all WebSocket connections immediately
    for connection in active_connections.copy():
        try:
            await connection.close()
        except:
            pass
    active_connections.clear()
    
    # Set shutdown event
    shutdown_event.set()
    
    # Give it 1 second then force kill
    asyncio.create_task(delayed_force_exit())
    
    return {"status": "shutting_down", "message": "Process will terminate in 1 second"}

async def delayed_force_exit():
    """Force exit after brief delay"""
    await asyncio.sleep(1.0)
    print("üõë FORCE KILLING PROCESS NOW")
    os._exit(0)  # Nuclear option - bypasses all cleanup

# ============================================================================
# MAIN ENTRY POINT 
# ============================================================================

if __name__ == "__main__":
    host = os.getenv("AIDE_HOST", config.get("host", "127.0.0.1"))
    port = int(os.getenv("AIDE_PORT", config.get("port", 8000)))
    
    print(f"üöÄ Starting FULLY OPTIMIZED AIDE on {host}:{port}")
    print(f"ü§ñ Models available: {len(safe_list_available_models())}")
    print(f"üéØ Current model: {CURRENT_MODEL if is_valid_model_path(CURRENT_MODEL) else 'None - will auto-select first available'}")
    print(f"üîå WebSocket: ‚úÖ FULLY OPTIMIZED startup")
    print(f"üõ†Ô∏è Dynamic tools: ‚úÖ Bulletproof registration system")
    print(f"‚ö° ALL OPTIMIZATIONS: Non-blocking lifespan, fixed model validation, lazy loading, enhanced fallbacks")
    
    uvicorn.run(app, host=host, port=port)
